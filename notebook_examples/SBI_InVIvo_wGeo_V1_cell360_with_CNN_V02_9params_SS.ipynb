{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "from sbi.inference import SNPE, prepare_for_sbi\n",
    "from sbi.utils.get_nn_models import posterior_nn\n",
    "import sbi.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stylized_module.stylized_cell import Stylized_Cell\n",
    "from stylized_module.ecp import EcpMod, newposition\n",
    "from stylized_module.recorder import Recorder\n",
    "\n",
    "#Remove classes and use templates\n",
    "class Cell(Stylized_Cell):\n",
    "    \"\"\"Define single cell model using parent class Stylized_Cell\"\"\"\n",
    "    def __init__(self,geometry=None,dL=30,vrest=-70.0):\n",
    "        \"\"\"\n",
    "        Initialize cell model\n",
    "        geometry: pandas dataframe of cell morphology properties\n",
    "        dL: maximum segment length\n",
    "        vrest: reversal potential of leak channel for all segments\n",
    "        \"\"\"\n",
    "        super().__init__(geometry,dL,vrest)\n",
    "        self.record_soma_v() # uncomment this if want to record soma voltage\n",
    "    \n",
    "    def set_channels(self,gl_soma=15e-5,gl_dend=1e-5):\n",
    "        \"\"\"Define biophysical properties, insert channels\"\"\"\n",
    "#         self.set_all_passive(gl=0.0003)  # soma,dend both have gl\n",
    "        for sec in self.all:\n",
    "            sec.cm = 1.0\n",
    "            sec.insert('pas')\n",
    "            sec.e_pas = self._vrest\n",
    "        self.soma.g_pas = 15e-5*5\n",
    "        for sec in self.all[1:]:\n",
    "            sec.g_pas = 1e-5*5\n",
    "    \n",
    "    def record_soma_v(self):\n",
    "        self.v_rec = Recorder(self.soma(.5),'v')\n",
    "    \n",
    "    def v(self):\n",
    "        \"\"\"Return recorded soma membrane voltage in numpy array\"\"\"\n",
    "        if hasattr(self,'v_rec'):\n",
    "            return self.v_rec.as_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuron import h\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "h.load_file('stdrun.hoc')\n",
    "\n",
    "class Simulation(object):\n",
    "    def __init__(self,geometry,electrodes,soma_injection,loc_param,geo_param=None,scale=1.0):\n",
    "        \"\"\"\n",
    "        Initialize simulation object\n",
    "        geometry: pandas dataframe of cell morphology properties\n",
    "        electrodes: array of electrode coordinates, n-by-3\n",
    "        soma_injection: vector of some injection waveform\n",
    "        loc_param: location parameters, ncell-by-4 array\n",
    "        geo_param: geometry parameters, ncell-by-k array, if not specified, use default properties in geometry\n",
    "        scale: scaling factors of lfp magnitude, ncell-vector, if is single value, is constant for all cells\n",
    "        \"\"\"\n",
    "        self.ncell = 0  # number of cells in this simulation\n",
    "        self.cells = []  # list of cell object\n",
    "        self.lfp = []  # list of EcpMod object\n",
    "        self.define_geometry_entries()  # list of entries to geometry dataframe\n",
    "        self.geometry = geometry.copy()\n",
    "        self.electrodes = electrodes\n",
    "        self.soma_injection = soma_injection\n",
    "        self.set_loc_param(loc_param)  # setup variable location parameters\n",
    "        self.set_geo_param(geo_param)  # setup variable geometry parameters\n",
    "        self.set_scale(scale)  # setup scaling factors of lfp magnitude\n",
    "        self.create_cells()  # create cell objects with properties set up\n",
    "        self.t_vec = h.Vector( round(h.tstop/h.dt)+1 ).record(h._ref_t)  # record time\n",
    "    \n",
    "    def set_loc_param(self,loc_param):\n",
    "        \"\"\"Setup location parameters. loc_param ncell-by-4 array\"\"\"\n",
    "        loc_param = np.array(loc_param)\n",
    "        if loc_param.ndim==1:\n",
    "            loc_param = np.expand_dims(loc_param,0)\n",
    "        self.ncell = loc_param.shape[0]\n",
    "        self.loc_param = [(np.insert(loc_param[i,:2],2,0.),loc_param[i,2:]) for i in range(self.ncell)]\n",
    "    \n",
    "    def set_geo_param(self,geo_param):\n",
    "        \"\"\"Setup geometry parameters. geo_param ncell-by-k array, k entries of properties\"\"\"\n",
    "        if geo_param is None:\n",
    "            self.geo_param = None\n",
    "        else:\n",
    "            geo_param = np.array(geo_param)\n",
    "            if geo_param.ndim==1:\n",
    "                geo_param = np.expand_dims(geo_param,0)\n",
    "            if geo_param.shape[0]!=self.ncell:\n",
    "                raise ValueError(\"geo_param number of rows does not match loc_param\")\n",
    "            self.geo_param = geo_param\n",
    "    \n",
    "    def set_scale(self,scale):\n",
    "        if not hasattr(scale,'__len__'):\n",
    "            self.scale = np.full(self.ncell,scale)\n",
    "        else:\n",
    "            scale = np.array(scale).ravel()\n",
    "            if scale.size!=self.ncell:\n",
    "                raise ValueError(\"scale size does not match loc_param rows\")\n",
    "            self.scale = scale\n",
    "    \n",
    "    def define_geometry_entries(self):\n",
    "        \"\"\"Define list of entries to geometry dataframe. Each entry is a pair of section id and property.\"\"\"\n",
    "        self.geo_entries = [\n",
    "            (0,'R'),  # change soma radius\n",
    "            (3,'L'),  # change trunk length\n",
    "            (3,'R'),  # change trunk radius\n",
    "            ([1,2],'R'),  # change dendrites radius\n",
    "            (4,'R'),  # change tuft radius\n",
    "            ([1,2,4],'L') # change dendrite length\n",
    "        ]\n",
    "    \n",
    "    def set_geometry(self,geometry,geo_param):\n",
    "        \"\"\"Set property values from geo_param through each entry to geometry. Return dataframe\"\"\"\n",
    "        geom = geometry.copy()\n",
    "        for i,x in enumerate(geo_param):\n",
    "            if x>=0:\n",
    "                geom.loc[self.geo_entries[i]] = x\n",
    "        return geom\n",
    "    \n",
    "    def create_cells(self):\n",
    "        \"\"\"Create cell objects with properties set up\"\"\"\n",
    "        self.cells.clear()  # remove cell objects from previous run\n",
    "        self.lfp.clear()\n",
    "        if self.geo_param is None:\n",
    "            for i in range(self.ncell):\n",
    "                self.cells.append( Cell(geometry=self.geometry) )\n",
    "        else:\n",
    "            for i in range(self.ncell):\n",
    "                geometry = self.set_geometry(self.geometry,self.geo_param[i,:])\n",
    "                self.cells.append( Cell(geometry=geometry) )\n",
    "        # add soma injection current and set up lfp recording\n",
    "        for i,cell in enumerate(self.cells):\n",
    "            cell.add_injection(sec_index=0,pulse=False,current=self.soma_injection,record=True)\n",
    "            # Move cell location\n",
    "            self.lfp.append( EcpMod(cell,self.electrodes,move_cell=self.loc_param[i],scale=self.scale[i]) )\n",
    "#             # Alternatively, if number of segments in the cell is much larger then electrodes, then move the electrodes instead\n",
    "#             translate, rotate = self.loc_param[i]\n",
    "#             self.lfp.append( EcpMod(cell,newposition(translate,rotate,self.electrodes,move_frame=True),scale=self.scale[i]) )\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run simulation\"\"\"\n",
    "        h.run()\n",
    "    \n",
    "    def t(self):\n",
    "        \"\"\"Return simulation time vector\"\"\"\n",
    "        return self.t_vec.as_numpy()\n",
    "    \n",
    "    def get_lfp(self,index=0):\n",
    "        \"\"\"Return LFP array of the cell by index, channels-by-time\"\"\"\n",
    "        return self.lfp[index].calc_ecp()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import in vivo trace and average to be 96 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cell360LFP.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f70a1de8edd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mIVTrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cell360LFP.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mAvgTraces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIVTrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIVTrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/sbi/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/sbi/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/sbi/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/sbi/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/sbi/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/sbi/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/sbi/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cell360LFP.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "IVTrace = np.array(pd.read_csv('cell360LFP.txt',header=None)).T #read in from h5 instead\n",
    "AvgTraces = np.mean(IVTrace.reshape(IVTrace.shape[0],4,96,order='F'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_iv = 1/30\n",
    "t_iv = dt_iv*np.arange(AvgTraces.shape[0])  # times of 30kHz\n",
    "dt = 0.025  # 40kHz\n",
    "t = np.arange(0.,t_iv[-1],dt)  # times of 40kHz\n",
    "newAvgTraces = np.empty((t.size,AvgTraces.shape[1]))\n",
    "for i in range(AvgTraces.shape[1]):  # interpolate for each channel\n",
    "    newAvgTraces[:,i] = np.interp(t,t_iv,AvgTraces[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "hf = h5py.File('cell360LFP.h5', 'w')\n",
    "hf.create_dataset('data',data=newAvgTraces)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.spike_window import first_pk_tr, get_spike_window\n",
    "\n",
    "nleading = 10\n",
    "fst_idx = first_pk_tr(newAvgTraces)\n",
    "idx = slice(*get_spike_window(newAvgTraces,win_size=96,align_at=nleading))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_results import plot_LFP_traces,plot_LFP_heatmap\n",
    "%matplotlib inline\n",
    "\n",
    "y_dist = np.linspace(-1900,1900,96)\n",
    "plot_LFP_traces(t[idx],newAvgTraces[idx,:])\n",
    "plot_LFP_heatmap(t[idx],y_dist,newAvgTraces[idx,:],vlim=[-20,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 289\n",
    "end = 385\n",
    "dt = 1/30\n",
    "IVTrace = np.array(pd.read_csv('cell360LFP.txt',header=None)).T\n",
    "AvgTraces = np.mean(IVTrace.reshape(IVTrace.shape[0],4,96,order='F'),axis=1)  # easier way to average every 4 channels\n",
    "time = np.linspace(start*dt,end*dt,end-start,endpoint=False)  # do not include the endpoint, so that the time interval is correct\n",
    "IVTraces = AvgTraces[start:end,:]\n",
    "maxIndx = np.argmax(np.absolute(AvgTraces).max(axis=0))  # find maximum absolute value from averaged traces\n",
    "maxTrace = -IVTraces[:,maxIndx] + 20\n",
    "maxArg = np.argmax(maxTrace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make default electrode coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,0,96)#np.zeros(a)#16 um apart\n",
    "y = np.linspace(-1900,1900,96)\n",
    "z = np.zeros(96)\n",
    "pos = np.vstack((x,y,z)).T\n",
    "posdf = pd.DataFrame(pos)\n",
    "pos = np.array(posdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Begin setup the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuron import h\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load geometry into pandas dataframe\n",
    "geo_standard = pd.read_csv('geom_standard.csv',index_col='id')\n",
    "# Load electrode coordinates (this is just for example. update it to be 96*3)\n",
    "#elec_pos = pd.read_csv('linear_electrode.csv',sep=' ')[[x+'_pos' for x in list('xyz')]].values\n",
    "elec_pos = posdf\n",
    "# Basic settings for simulation\n",
    "h.load_file('stdrun.hoc')\n",
    "h.tstop = 4.\n",
    "h.dt = 1/30\n",
    "h.v_init = -70.0  # should be close to resting potential\n",
    "\n",
    "# define example current injection waveform (later use in vivo LFP waveform in the channel with maximum amplitude)\n",
    "t = np.arange(0,h.tstop+h.dt,h.dt)\n",
    "t_inj = np.arange(5,15,h.dt)\n",
    "#soma_injection = np.zeros(t.shape)\n",
    "soma_injection = np.insert(maxTrace,0,0.)\n",
    "#idx = int(5/h.dt)\n",
    "#soma_injection[idx:idx+len(t_inj)] += 0.5*np.sin(2*np.pi*0.2*t_inj)  # simple sine wave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create simulation object. Here run with only one cell and only location changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_param = [0,0,0,1,0]\n",
    "geo_param = [4.0, -1, 0.2, -1, -1, 300.0]  # default value [6.0, 400.0, 0.1, 0.1, 0.1, 200.0]\n",
    "geo_param = np.tile(geo_param,(3,1))  # Repeat to get 3-by-6 array.\n",
    "sim = Simulation(geo_standard,elec_pos,soma_injection,loc_param,scale=100.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of codes below can be put into the simulator function in sbi, to be run multiple times. You can run one cell at a time and run through all samples in series. Or you can run multiple cells at once, and gather the results as presimulated data for each round in sbi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(param,t):\n",
    "    \n",
    "    from scipy import signal\n",
    "    \n",
    "   # sim.set_loc_param(loc_param[0][:4])#for test\n",
    "   # sim.set_scale(loc_param[0][4])\n",
    "    #for range from prior()\n",
    "    # default value [6.0, 400.0, 0.1, 0.1, 0.1, 200.0]\n",
    "    \n",
    "    sim.set_loc_param(param[:5])\n",
    "    scalVal = 10 ** param[5]\n",
    "    sim.set_scale(scalVal)\n",
    "    #gparamarray = np.array([6.0, 400.0, 0.1, 0.1, 0.1, 200.0])#[param[5],param[6], param[7], 0.1, 0.1, 200.0])\n",
    "    #sim.set_geo_param(gparamarray)\n",
    "    sim.create_cells()\n",
    "    sim.run()\n",
    "    lfp = sim.get_lfp()\n",
    "    lfp = lfp.T\n",
    "    b,a = signal.butter(2,100,'hp',fs=30000)    # order 2, 100 Hz, highpass, sampling rate = 40kHz\n",
    "    filtered_lfp = signal.lfilter(b,a,lfp,axis=0)    # filter along row of the lfp 2d-array, if each row is a channel\n",
    "    filtered_lfp = filtered_lfp[0:96]\n",
    "        ###calculate lfp from current recordings\n",
    "    simDict = {\"traces\": filtered_lfp,\n",
    "              \"time\": t}\n",
    "    return simDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somalfp = model([10,0,0,1,0,1,-1,-1,-1],time)['traces'][:,47]\n",
    "#sim.run()\n",
    "#somalfp = sim.get_lfp()[48,:]\n",
    "#somalfp = somalfp[1:]\n",
    "plt.plot(time,somalfp)\n",
    "print(time[np.argmin(somalfp)])\n",
    "print(time[maxArg])\n",
    "shift = maxArg - np.argmin(somalfp)\n",
    "print(shift)\n",
    "#sim = Simulation(geo_standard,elec_pos,soma_injection,loc_param)\n",
    "sim = Simulation(geo_standard,elec_pos,soma_injection,loc_param,scale=100.)  # LFP scaled by 100 times for all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "#import torch\n",
    "##################################for CNNN\n",
    "#def simulation(params):\n",
    "#    s = model(params,t)\n",
    " #   return torch.from_numpy(np.ravel(s['traces']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "class SummaryNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 2D convolutional layer, input 96, with kernel_size 35, output 92x92\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3)\n",
    "        # Maxpool layer that reduces 96x96 image to 46x46\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)  # do not need to specify stride, by default stride=kernel_size\n",
    "        \n",
    "        # 2D convolutional layer, input 46x46, with kernel_size 3, output 44x44\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3)\n",
    "        # Maxpool layer that reduces 44x44 image to 22x22\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # 2D convolutional layer, input 22x22, with kernel_size 3, output 20x20\n",
    "        self.conv3 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3)\n",
    "        # Maxpool layer that reduces 20x20 image to 10x10\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected layer taking as input the 8 flattened output arrays from the maxpooling layer\n",
    "        self.fc = nn.Linear(in_features=800, out_features=20) # 8*10*10=800\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,1,96,96) # (batch size,in_channels,height,length) -1 means not changing size of that dimension\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1,800) # (batch size, in_features)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return x\n",
    "\n",
    "embedding_net = SummaryNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_position = pos\n",
    "def statscalc(sumStat):\n",
    "    \n",
    "    mean_ch_vals = np.mean(sumStat)\n",
    "    stdDev_ch_vals = np.std(sumStat)\n",
    "    max_ch_val = np.array(max(sumStat)).reshape(-1)\n",
    "    min_ch_val = np.array(min(sumStat)).reshape(-1)\n",
    "\n",
    "    \n",
    "    max_ch_pos = np.array(np.argmax(sumStat)).reshape(-1)#number of the channel at which max v occurs\n",
    "    min_ch_pos = np.array(np.argmin(sumStat)).reshape(-1)\n",
    "       \n",
    "\n",
    "    y = old_position[max_ch_pos,1]\n",
    "    orig_posMax = np.array([y]).reshape(-1)\n",
    "    \n",
    "    y = old_position[min_ch_pos,1]\n",
    "    orig_posMin = np.array([y]).reshape(-1)\n",
    "    \n",
    "    All = np.hstack((orig_posMax,orig_posMin, mean_ch_vals, stdDev_ch_vals,max_ch_val,min_ch_val))\n",
    "    \n",
    "    return(All)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats as spstats\n",
    "\n",
    "def Stats(results):\n",
    "    \"\"\"\n",
    "    Calculates summary statistics\n",
    "    results = model(params)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    X = np.array(results['traces'])\n",
    "    [a,b] = X.shape\n",
    "    T = np.array(results['time'])\n",
    "    \n",
    "    avg = np.mean(X, axis = 0)# average voltage of each channel\n",
    "    stdDev = np.std(X, axis = 0)# stDev of the voltage of each channel\n",
    "\n",
    "    maxTrough = []\n",
    "    timesT = []\n",
    "    maxPeak = []\n",
    "    timesP = [] \n",
    "   \n",
    "    for i in range(0,b):\n",
    "        maxPeak.append(max(X[:,i]))# max voltage of channel i\n",
    "        timesP.append(T[np.argmax(X[:,i])]) # time at which the max sample was taken in channel i\n",
    "        maxTrough.append(min(X[:,i]))# min voltage of channel i\n",
    "        timesT.append(T[np.argmin(X[:,i])]) # time at which the min sample was taken in channel i\n",
    "    avg = np.array(avg).reshape(-1)   \n",
    "    stdDev = np.array(stdDev).reshape(-1)          \n",
    "    tPeak_Trough = np.array(timesP) - np.array(timesT)       \n",
    "\n",
    "\n",
    "    maxStats = np.array(statscalc(maxPeak)).reshape(-1)\n",
    "    minStats = np.array(statscalc(maxTrough)).reshape(-1)\n",
    "    avgStats = np.array(statscalc(avg)).reshape(-1) \n",
    "    relTimeStats = np.array(statscalc(tPeak_Trough)).reshape(-1)\n",
    "\n",
    "  \n",
    "            \n",
    "    allStats = np.concatenate((avgStats,maxStats,relTimeStats, minStats,))\n",
    "    \n",
    "\n",
    "    return np.asarray(allStats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "\n",
    "def simulation(params):\n",
    "    s = Stats(model(params,t))\n",
    "    return torch.from_numpy(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lows = torch.tensor([0,-2000, 0,-1,0,3,3,100,.05],dtype = float)\n",
    "Highs = torch.tensor([200,2000,np.pi,1,np.pi,5,10,800,0.2],dtype = float)\n",
    "\n",
    "prior = utils.BoxUniform(low=Lows, high= Highs)\n",
    "\n",
    "#as a reminder, the prior is also defined earlier\n",
    "ranParams = prior.sample()#example sample\n",
    "ranParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi import inference\n",
    "import torch\n",
    "from sbi.inference import SNPE, prepare_for_sbi, simulate_for_sbi\n",
    "from sbi.utils.get_nn_models import posterior_nn\n",
    "from sbi import utils as utils\n",
    "torch.manual_seed(0)\n",
    "# instantiate the neural density estimator\n",
    "neural_posterior = utils.posterior_nn(model='maf', \n",
    "                                      #embedding_net=embedding_net,\n",
    "                                      hidden_features=10,\n",
    "                                      num_transforms=2)\n",
    "\n",
    "simulator, prior = prepare_for_sbi(simulation, prior)\n",
    "# setup the inference procedure with the SNPE-C procedure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IVTrace_x0 = AvgTraces[start+shift:end+shift,:]\n",
    "results = {\"traces\": IVTrace_x0,\n",
    "          \"time\" : time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2 rounds: first round simulates from the prior, second round simulates from the obtained posterior.\n",
    "num_rounds = 3\n",
    "# The specific observation we want to focus the inference on.\n",
    "x_o = Stats(results)#torch.tensor(np.ravel(IVTrace_x0))\n",
    "\n",
    "# setup the inference procedure with the SNPE-C procedure\n",
    "inference = SNPE(prior=prior, density_estimator=neural_posterior)\n",
    "posteriors = []\n",
    "proposal = prior\n",
    "num_rounds=3\n",
    "for _ in range(num_rounds):\n",
    "    theta, x = simulate_for_sbi(simulator, proposal, num_simulations=2000)\n",
    "    \n",
    "     # In `SNLE` and `SNRE`, you should not pass the `proposal` to `.append_simulations()`\n",
    "    density_estimator = inference.append_simulations(theta, x, proposal=proposal).train()\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    posteriors.append(posterior)\n",
    "    proposal = posterior.set_default_x(x_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_o = torch.tensor(np.ravel(results['traces']))\n",
    "x_o = x_o.reshape(1,96*96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_trace = x_o\n",
    "observation_summary_statistics = observation_trace\n",
    "labels_params = [ r'x',r'y',r'theta',r'h', r'phi',r'scale',r'somaR',r'TrunkL',r'TrunkR']\n",
    "samples = posterior.sample((1000,), x=x_o)#,sample_with_mcmc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probability = posterior.log_prob(samples, x = x_o)\n",
    "predicted_post_index = np.argmin(log_probability)\n",
    "predicted_post = samples[predicted_post_index]\n",
    "plt.hist(log_probability)\n",
    "print(\"predicted post is: \",predicted_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model(predicted_post,time)['time'],model(predicted_post,time)['traces'])\n",
    "\n",
    "plt.xlabel('ms', fontsize=40)\n",
    "plt.ylabel('LFP (\\u03bcV)', fontsize = 40,labelpad=-30)\n",
    "#plt.ylim([-50,50])\n",
    "plt.xticks([10, 11.5,13],fontsize = 40)\n",
    "plt.yticks([-100,0,100],fontsize = 40)\n",
    "\n",
    "plt.tick_params(length = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "#posterior.leakage_correction(observation_summary_statistics, num_rejection_samples=1000)\n",
    "log_probability = posterior.log_prob(samples, x=observation_summary_statistics,norm_posterior=False)\n",
    "fig, axes = utils.pairplot(samples,\n",
    "                           limits=[[0,200],[-2000,2000], [-1,1],[0,np.pi],[3,5],[3,10],[100,800],[.05,0.2]],#,[3,12]],\n",
    "                           ticks=[[0,200],[-2000,2000], [-1,1],[0,np.pi],[3,5],[3,10],[100,800],[.05,0.2]],#,[3,12]],\n",
    "                           fig_size=(15,15),\n",
    "                           #points=true_params,\n",
    "                           points_offdiag={'markersize': 6},\n",
    "                           labels = labels_params,\n",
    "                           points_colors='r');\n",
    "FS = 40\n",
    "\n",
    "\n",
    "axes[0][0].set_xlabel('x',fontsize = FS,zorder=2.5)\n",
    "axes[1][1].set_xlabel('y',fontsize = FS,zorder=2.5)\n",
    "axes[2][2].set_xlabel('h',fontsize = FS,zorder=2.5)\n",
    "axes[3][3].set_xlabel('\\u03C6',fontsize=FS )\n",
    "axes[4][4].set_xlabel('\\u03BB',fontsize = FS)\n",
    "axes[5][5].set_xlabel('R_s',fontsize = FS)\n",
    "axes[6][6].set_xlabel('L_t',fontsize=FS)\n",
    "axes[7][7].set_xlabel('R_t',fontsize=FS)\n",
    "for i in range(8):\n",
    "    axes[i][i].tick_params('x',labelsize=15,zorder=2.5)\n",
    "\n",
    "LS = 30\n",
    "axes[0][0].set_xticklabels(['0','200'],fontsize =LS)\n",
    "axes[1][1].set_xticklabels(['-2000','2000'],fontsize =LS,zorder=4)\n",
    "axes[2][2].set_xticklabels(['-1','1'],fontsize =LS)\n",
    "axes[3][3].set_xticklabels(['0','3.14'],fontsize =LS)\n",
    "axes[4][4].set_xticklabels(['3','5'],fontsize =LS)\n",
    "axes[5][5].set_xticklabels(['3','10'],fontsize =LS)\n",
    "axes[6][6].set_xticklabels(['100','800'],fontsize =LS)\n",
    "axes[7][7].set_xticklabels(['0.05','0.2'],fontsize =LS)\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig('/home/ddopp4877/Desktop/cell360_GEO_KDE.pdf', bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probability = posterior.log_prob(samples, x=observation_summary_statistics,norm_posterior=False)\n",
    "fig, axes = utils.pairplot(samples,\n",
    "                           limits=[[0,200],[-2000,2000], [0,np.pi],[-1,1],[0,np.pi],[3,5],[3,10],[100,800],[.05,0.2]],#,[3,12]],\n",
    "                           ticks=[[0,200],[-2000,2000], [0,np.pi],[-1,1],[0,np.pi],[3,5],[3,10],[100,800],[.05,0.2]],#,[3,12]],\n",
    "                           fig_size=(10,10),\n",
    "                           #points=true_params,\n",
    "                           points_offdiag={'markersize': 6},\n",
    "                           labels = labels_params,\n",
    "                           points_colors='r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = posterior.sample((1,))\n",
    "\n",
    "_ = conditional_pairplot(\n",
    "    density=posterior,\n",
    "    condition=condition,\n",
    "    limits=[[0,200],[-2000,2000], [-1,1],[0,np.pi],[3,5],[3,10],[100,800],[.05,0.2]],\n",
    "    fig_size=(5,5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model(predicted_post,time)['time'],model(predicted_post,time)['traces'])\n",
    "\n",
    "plt.xlabel('ms', fontsize=40)\n",
    "plt.ylabel('LFP (\\u03bcV)', fontsize = 40,labelpad=-30)\n",
    "\n",
    "plt.xticks([10, 11.5,13],fontsize = 40)\n",
    "plt.yticks([-50,0,50],fontsize = 40)\n",
    "\n",
    "plt.tick_params(length = 15)\n",
    "\n",
    "plt.savefig('/home/ddopp4877/Desktop/cell360GEOpred_Trace.pdf', bbox_inches='tight',transparent=True)\n",
    "print(\"parameters chosen by inference = \",predicted_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results['time'],results['traces'])\n",
    "\n",
    "plt.xlabel('ms', fontsize=40)\n",
    "plt.ylabel('LFP (\\u03bcV)', fontsize = 40,labelpad=-10)\n",
    "\n",
    "plt.xticks([10, 11.5,13],fontsize = 40)\n",
    "plt.yticks([-100,-50,0,50],fontsize = 40)\n",
    "\n",
    "plt.tick_params(length = 15)\n",
    "\n",
    "plt.savefig('/home/ddopp4877/Desktop/cell360IV_Trace.pdf', bbox_inches='tight',transparent=True)\n",
    "\n",
    "print(results['traces'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calculating correlation coefficient, a value between -1 and 1.\n",
    "def corrcoef(x,y):\n",
    "    return (np.mean(x*y)-np.mean(x)*np.mean(y))/(np.std(x)*np.std(y))\n",
    "\n",
    "# Calculate correlation coefficient between input x and y inside sliding time window\n",
    "# (time should be the first axis, i.e. a column is a channel)\n",
    "# Find the maximum correlation coefficient and the corresponding window location.\n",
    "# Return first the maximum value, then return the index of the start point of corresponding window.\n",
    "# If window_size is not specified, use the length of x as window size. Return the index of window in y.\n",
    "# If window_size is specified, slide the window separately in x and y. Return the indices of windows in x and y respectively.\n",
    "def max_corrcoef(x,y,window_size=None):\n",
    "    if window_size is None:\n",
    "        win = x.shape[0]\n",
    "    else:\n",
    "        win = window_size\n",
    "    nx = x.shape[0]-win+1\n",
    "    ny = y.shape[0]-win+1\n",
    "    corr = np.empty((nx,ny))\n",
    "    for i in range(nx):\n",
    "        for j in range(ny):\n",
    "            corr[i,j] = corrcoef(x[i:i+win,:].ravel(),y[j:j+win,:].ravel())\n",
    "    maxind = np.argmax(corr)\n",
    "    maxind = np.unravel_index(maxind,(nx,ny))\n",
    "    max_corr = corr[maxind]\n",
    "    if window_size is None:\n",
    "        output = (max_corr,maxind[1])\n",
    "    else:\n",
    "        output = (max_corr,maxind[0],maxind[1])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_corr,max_ind = max_corrcoef(results['traces'],model(postparams,time)['traces'])  # IVTraces has length 100, lfp has length 150, window size is 100\n",
    "print(max_corr)  # return correlation coefficient and the index of the start point of the window in lfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(results['traces'],model(postparams,time)['traces'])[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# heatmaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window=[4.8,5.2]\n",
    "window=[0,20]\n",
    "lfp =  model(predicted_post,time)['traces']\n",
    "T= time\n",
    "idx = (T>=window[0]) & (T<=window[1])\n",
    "lfp_m = [np.min(lfp),np.max(lfp)]\n",
    "lfp_m = [-10,10]\n",
    "elec_d = pos[:,1] # electrode location (y-coordinate)\n",
    "TT = T[idx]\n",
    "lfp = lfp[idx]\n",
    "lfp = lfp.transpose()\n",
    "fig, ax = plt.subplots()\n",
    "pcm = plt.pcolormesh(TT,elec_d/1000,lfp,cmap='viridis',vmin=lfp_m[0],vmax=lfp_m[1])\n",
    "cbaxes = fig.add_axes([.91,0.118,.03,0.76])\n",
    "cbar = fig.colorbar(pcm,ax=ax,ticks=[-10,0,10],cax = cbaxes)\n",
    "\n",
    "\n",
    "FS = 40\n",
    "NS = 30\n",
    "\n",
    "cbar.set_label('LFP (\\u03bcV)',fontsize=FS,labelpad=-12)\n",
    "cbar.set_ticklabels(['-10','0','10'])\n",
    "cbar.ax.tick_params(labelsize=NS)\n",
    "#cbar.tick_params(['-10','0','10'],labelsize=15)\n",
    "\n",
    "ax.set_xlabel('time (ms)',fontsize=FS)\n",
    "ax.set_ylabel('dist_y (mm)',fontsize=FS)\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xticks([10,11.5,13])\n",
    "ax.set_xticklabels(['10.0','11.5','13'],fontsize=NS)\n",
    "\n",
    "ax.set_yticks([-1.5,0,1.5])\n",
    "ax.set_yticklabels(['-1.5','0','1.5'],fontsize=NS)\n",
    "\n",
    "\n",
    "plt.savefig('/home/ddopp4877/Desktop/cell360GEO_predHTMP.pdf', bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window=[4.8,5.2]\n",
    "window=[0,20]\n",
    "lfp =  results['traces']\n",
    "T= time\n",
    "idx = (T>=window[0]) & (T<=window[1])\n",
    "lfp_m = [np.min(lfp),np.max(lfp)]\n",
    "lfp_m = [-10,10]\n",
    "elec_d = pos[:,1] # electrode location (y-coordinate)\n",
    "TT = T[idx]\n",
    "lfp = lfp[idx]\n",
    "lfp = lfp.transpose()\n",
    "fig, ax = plt.subplots()\n",
    "pcm = plt.pcolormesh(TT,elec_d/1000,lfp,cmap='viridis',vmin=lfp_m[0],vmax=lfp_m[1])\n",
    "cbaxes = fig.add_axes([.91,0.118,.03,0.76])\n",
    "cbar = fig.colorbar(pcm,ax=ax,ticks=[-10,0,10],cax = cbaxes)\n",
    "\n",
    "\n",
    "FS = 40\n",
    "NS = 30\n",
    "\n",
    "cbar.set_label('LFP (\\u03bcV)',fontsize=FS,labelpad=-12)\n",
    "cbar.set_ticklabels(['-10','0','10'])\n",
    "cbar.ax.tick_params(labelsize=NS)\n",
    "#cbar.tick_params(['-10','0','10'],labelsize=15)\n",
    "\n",
    "ax.set_xlabel('time (ms)',fontsize=FS)\n",
    "ax.set_ylabel('dist_y (mm)',fontsize=FS)\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xticks([10,11.5,13])\n",
    "ax.set_xticklabels(['10.0','11.5','13'],fontsize=NS)\n",
    "\n",
    "ax.set_yticks([-1.5,0,1.5])\n",
    "ax.set_yticklabels(['-1.5','0','1.5'],fontsize=NS)\n",
    "\n",
    "\n",
    "plt.savefig('/home/ddopp4877/Desktop/cell360IV_HTMP.pdf', bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
