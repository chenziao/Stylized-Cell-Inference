{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "central-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuron import h\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import json\n",
    "from scipy import signal\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "from typing import Union, List, Tuple, Dict\n",
    "import os\n",
    "\n",
    "from cell_inference.config import paths, params\n",
    "from cell_inference.cells.simulation import Simulation\n",
    "from cell_inference.cells.stylizedcell import CellTypes\n",
    "from cell_inference.utils.transform.distribution_transformation import range2norm, range2logn\n",
    "from cell_inference.utils.transform.geometry_transformation import pol2cart, cart2pol\n",
    "from cell_inference.utils.spike_window import first_pk_tr, get_spike_window\n",
    "from cell_inference.utils.plotting.plot_results import plot_lfp_traces, plot_lfp_heatmap\n",
    "from cell_inference.utils.feature_extractors.SummaryStats2D import calculate_stats, build_lfp_grid\n",
    "from cell_inference.utils.feature_extractors.parameterprediction import ClassifierTypes, ClassifierBuilder\n",
    "\n",
    "\n",
    "h.load_file('stdrun.hoc')\n",
    "h.nrn_load_dll(paths.COMPILED_LIBRARY)\n",
    "geo_standard = pd.read_csv(paths.GEO_STANDARD, index_col='id')\n",
    "h.tstop = params.TSTOP\n",
    "h.dt = params.DT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4949f3",
   "metadata": {},
   "source": [
    "### Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e836cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'cell_inference/resources/simulation_data'\n",
    "TRIAL_PATH = os.path.join(DATA_PATH, 'Ori3_params')\n",
    "\n",
    "TRIAL_CONFIG_PATH = os.path.join(TRIAL_PATH, 'config.json')  # trial configuration\n",
    "LFP_PATH = os.path.join(TRIAL_PATH, 'lfp.npz')  # LFP and labels\n",
    "MEM_VOLT_PATH = os.path.join(TRIAL_PATH, 'mem_volt.npz')  # membrane voltage and spike times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cecbc7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_config = open(TRIAL_CONFIG_PATH, 'r')\n",
    "config_dict = json.load(f_config)\n",
    "f_config.close()\n",
    "\n",
    "loc_param_list = config_dict['Simulation_Parameters']['loc_param_list']\n",
    "geo_param_list = config_dict['Simulation_Parameters']['geo_param_list']\n",
    "loc_param_default = config_dict['Simulation_Parameters']['loc_param_default']\n",
    "geo_param_default = config_dict['Simulation_Parameters']['geo_param_default']\n",
    "loc_param_range = config_dict['Simulation_Parameters']['loc_param_range']\n",
    "geo_param_range = config_dict['Simulation_Parameters']['geo_param_range']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb9de5d",
   "metadata": {},
   "source": [
    "Check configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "670e182c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc_param_list:  [\"x\", \"y\", \"z\", \"alpha\", \"h\", \"phi\"]\n",
      "geo_param_list:  [\"r_s\", \"l_t\", \"r_t\", \"r_d\", \"r_tu\", \"l_d\"]\n",
      "loc_param_default:  {\"x\": 0.0, \"y\": 0.0, \"z\": 50.0, \"alpha\": 0.7853981633974483, \"h\": 1.0, \"phi\": 0.0, \"d\": 50.0, \"theta\": 0.0, \"dvx\": [0.0], \"dvy\": [1.0], \"dvz\": [0.0]}\n",
      "geo_param_default:  {\"r_s\": 8.0, \"l_t\": 600.0, \"r_t\": 1.25, \"r_d\": 0.28, \"r_tu\": 0.28, \"l_d\": 200.0}\n",
      "loc_param_range:  {\"x\": [-50, 50], \"y\": [-1400, 1400], \"z\": [20.0, 200.0], \"alpha\": [0, 3.141592653589793], \"h\": [-1.0, 1.0], \"phi\": [-3.141592653589793, 3.141592653589793], \"d\": [20.0, 200.0], \"theta\": [-1.0471975511965976, 1.0471975511965976], \"dvx\": [-50, 50], \"dvy\": [-50, 50], \"dvz\": [-50, 50]}\n",
      "geo_param_range:  {\"r_s\": [7.0, 12.0], \"l_t\": [20.0, 800.0], \"r_t\": [0.6, 1.8], \"r_d\": [0.1, 0.8], \"r_tu\": [0.1, 0.8], \"l_d\": [100.0, 300.0]}\n",
      "loc_param_dist:  {\"x\": \"unif\", \"y\": \"unif\", \"z\": \"unif\", \"alpha\": \"unif\", \"h\": \"unif\", \"phi\": \"unif\", \"d\": \"unif\", \"theta\": \"norm\", \"dvx\": \"unif\", \"dvy\": \"unif\", \"dvz\": \"unif\"}\n",
      "geo_param_dist:  {\"r_s\": \"unif\", \"l_t\": \"unif\", \"r_t\": \"unif\", \"r_d\": \"logn\", \"r_tu\": \"logn\", \"l_d\": \"unif\"}\n"
     ]
    }
   ],
   "source": [
    "for key, value in config_dict['Simulation_Parameters'].items():\n",
    "    print(key+\": \", json.dumps(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cca8b90",
   "metadata": {},
   "source": [
    "### Set parameters to investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "811c41f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "SINGLE_PARAM = False  # Study single parameter. Otherwise, use generated samples in simulated data. \n",
    "\n",
    "if SINGLE_PARAM:\n",
    "    number_samples = 100  # number of samples\n",
    "    check_param = 'r_s'  # single parameter to vary\n",
    "    check_range = None  # or a tuple of range. Samples will be linearly spaced within the range\n",
    "    set_default = {}  # specify fixed parameters if any different from loaded configuration, e.g. {'r_s': 10.}\n",
    "else: \n",
    "    sample_idx = np.arange(0, 100, 1)  # sample indices to investigate\n",
    "    sample_idx = np.array(sample_idx).ravel()\n",
    "    number_samples = sample_idx.size\n",
    "    print(number_samples)\n",
    "    check_param = None\n",
    "    check_range = None\n",
    "    set_default = {}\n",
    "    with np.load(LFP_PATH) as lfp_data:\n",
    "        labels = lfp_data['y']\n",
    "        rand_param = lfp_data['rand_param']\n",
    "        gmax = lfp_data['gmax']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721610cf",
   "metadata": {},
   "source": [
    "#### Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e209587",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SINGLE_PARAM:\n",
    "    gmax_mapping = h5py.File(paths.GMAX_MAPPING, 'r')\n",
    "\n",
    "    geo_list = [geo_param_list[idx] for idx in gmax_mapping['settings/geo_index']]\n",
    "    for i, key in enumerate(geo_list):\n",
    "        geo_range = gmax_mapping['settings/geo_range'][i,:].copy()\n",
    "        geo_range[0] = max(geo_param_range[key][0], geo_range[0])\n",
    "        geo_range[1] = min(geo_param_range[key][1], geo_range[1])\n",
    "        geo_param_range[key] = tuple(geo_range) \n",
    "\n",
    "    squared_soma_radius = gmax_mapping['mapping'].attrs['squared_soma_radius']\n",
    "\n",
    "    # Use linear interpolation\n",
    "    gmax_interp = LinearNDInterpolator(gmax_mapping['mapping/geometry'][()], gmax_mapping['mapping/gmax'][()])\n",
    "\n",
    "    gmax_mapping.close()\n",
    "\n",
    "    # # Use linear regression\n",
    "    # clf = ClassifierBuilder()\n",
    "    # clf.load_clf(paths.RESOURCES_ROOT + \"gmax_lin_reg_classifier.joblib\")\n",
    "\n",
    "    def pred_gmax(geo_samples: Dict):\n",
    "        geo = []\n",
    "        for key in geo_list:\n",
    "            if squared_soma_radius and key == 'r_s':\n",
    "                geo.append(geo_samples[key]**2)\n",
    "            else:\n",
    "                geo.append(geo_samples[key])\n",
    "        gmax = gmax_interp(np.column_stack(geo))\n",
    "    #     gmax = clf.predict(np.column_stack(geo))\n",
    "        return gmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b943946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([])\n"
     ]
    }
   ],
   "source": [
    "set_default_list = set_default.keys()\n",
    "for param_default in [loc_param_default,geo_param_default]:\n",
    "    for key in param_default.keys():\n",
    "        if key in set_default_list:\n",
    "            param_default[key] = set_default[key]\n",
    "\n",
    "if 'x' in set_default_list or 'z' in set_default_list:\n",
    "    loc_param_default['d'], loc_param_default['theta'] = cart2pol(loc_param_default['x'], loc_param_default['z'])\n",
    "\n",
    "if 'd' in set_default_list or 'theta' in set_default_list:\n",
    "    loc_param_default['x'], loc_param_default['z'] = pol2cart(loc_param_default['d'], loc_param_default['theta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4648316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_parameters(num: int, param_keys: List[str], param_default: List[float], \n",
    "                        param_range: List[Union[List[float],Tuple[float],np.ndarray]]):\n",
    "    global check_range\n",
    "    array_size = num\n",
    "    param_array = {}\n",
    "    for key in param_keys:\n",
    "        if key == check_param:\n",
    "            if check_range is None:\n",
    "                check_range = param_range[key]\n",
    "            param_array[key] = np.linspace(check_range[0], check_range[1], num=np.prod(array_size)).reshape(array_size)\n",
    "        else:\n",
    "            param_array[key] = np.full(array_size, param_default[key])\n",
    "    return param_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58b9506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_param_gen = loc_param_list.copy()\n",
    "if check_param in ['d', 'theta']:\n",
    "    loc_param_gen[loc_param_gen.index('x')] = 'd'\n",
    "    loc_param_gen[loc_param_gen.index('z')] = 'theta'\n",
    "\n",
    "loc_param_samples = generate_parameters(number_samples, loc_param_gen, loc_param_default, loc_param_range)\n",
    "\n",
    "if check_param in ['d', 'theta']:\n",
    "    loc_param_samples['x'], loc_param_samples['z'] = pol2cart(loc_param_samples['d'],loc_param_samples['theta'])\n",
    "\n",
    "loc_param = np.column_stack([loc_param_samples[key] for key in loc_param_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4fa3b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_param_samples = generate_parameters(number_samples, geo_param_list, geo_param_default, geo_param_range)\n",
    "\n",
    "geo_param = np.column_stack([geo_param_samples[key] for key in geo_param_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e84b095e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-94624f3d9464>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mrand_param_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mall_param\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mall_param\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand_param_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrand_param\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mloc_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_param\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc_param_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "if SINGLE_PARAM:\n",
    "    gmax = pred_gmax(geo_param_samples)\n",
    "else:\n",
    "    inference_list = config_dict['Trial_Parameters']['inference_list']\n",
    "    randomized_list = config_dict['Trial_Parameters']['randomized_list'][:-len(inference_list)]\n",
    "    all_param = np.hstack((loc_param,geo_param))\n",
    "    labels_idx = []\n",
    "    rand_param_idx = []\n",
    "    for i, key in enumerate(loc_param_list+geo_param_list):\n",
    "        if key in inference_list:\n",
    "            labels_idx.append(i)\n",
    "        elif key in randomized_list:\n",
    "            rand_param_idx.append(i)\n",
    "    print(labels_idx)\n",
    "    all_param[:,np.asarray(labels_idx)] = labels[sample_idx]\n",
    "    all_param[:,np.asarray(rand_param_idx)] = rand_param[sample_idx]\n",
    "    loc_param = all_param[:,:len(loc_param_list)]\n",
    "    geo_param = all_param[:,len(loc_param_list):]\n",
    "    gmax = gmax[sample_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61510303",
   "metadata": {},
   "source": [
    "#### Check parameters of all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_param = pd.DataFrame(np.column_stack((loc_param,geo_param,gmax)), columns=loc_param_list+geo_param_list+['gmax'])\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "display(all_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01442406",
   "metadata": {},
   "source": [
    "### Create simulation and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b14bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = Simulation(geometry = geo_standard, \n",
    "                 electrodes = params.ELECTRODE_POSITION, \n",
    "                 cell_type = CellTypes.ACTIVE, \n",
    "                 loc_param = loc_param, \n",
    "                 geo_param = geo_param,\n",
    "                 spike_threshold = -30, \n",
    "                 gmax = gmax, \n",
    "                 scale = 1., \n",
    "                 ncell = number_samples)\n",
    "\n",
    "sim.run_neuron_sim()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6347b0",
   "metadata": {},
   "source": [
    "Check invalid spiking cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.figure import Figure\n",
    "from matplotlib.axes import Axes\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_v(sim: Simulation, cell_idx: np.ndarray = None,\n",
    "           figsize: Union[List[float],Tuple[float]] = (6,2)) -> Tuple[Figure, Axes]:\n",
    "    if cell_idx is None:\n",
    "        cell_idx = np.arange(sim.ncell)\n",
    "    elif cell_idx.size == 0:\n",
    "        return\n",
    "    t = sim.t()\n",
    "    fig, axs = plt.subplots(nrows=cell_idx.size, ncols=1)\n",
    "    fig.set_size_inches(figsize[0],figsize[1]*cell_idx.size)\n",
    "    axs = np.asarray(axs).ravel()\n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.plot(t, sim.v(cell_idx[i]), label=\"cell %d\" % (cell_idx[i]))\n",
    "        ax.set_ylabel('Vm (mV)')\n",
    "        ax.legend(loc=1)\n",
    "    axs[0].set_title('Membrane Voltage vs Time')\n",
    "    axs[-1].set_xlabel('Time (ms)')\n",
    "    plt.show()\n",
    "    return fig, axs\n",
    "\n",
    "def invalid_index(sim):\n",
    "    # index of valid spiking cells\n",
    "    nspk, tspk = sim.get_spike_number('all')\n",
    "    invalid = np.nonzero(nspk != 1)[0]\n",
    "    return invalid, tspk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43a9f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_idx, _ = invalid_index(sim)\n",
    "print(\"Number of invalid samples: %d out of %d\" % (invalid_idx.size, number_samples))\n",
    "\n",
    "_ = plot_v(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e7df0c",
   "metadata": {},
   "source": [
    "### Get LFPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp = sim.get_lfp('all').transpose((0,2,1))  # (cells x channels x time) -> (cells x time x channels)\n",
    "\n",
    "filt_b, filt_a = signal.butter(params.BUTTERWORTH_ORDER,\n",
    "                              params.FILTER_CRITICAL_FREQUENCY,\n",
    "                              params.BANDFILTER_TYPE,\n",
    "                              fs=params.FILTER_SAMPLING_RATE)\n",
    "\n",
    "filtered_lfp = signal.lfilter(filt_b,filt_a,lfp,axis=1)  # filter along time axis\n",
    "\n",
    "pk_tr_idx_in_window = 16  # 16*0.025=0.4 ms\n",
    "lfp_list = []\n",
    "window_range = []\n",
    "for i in range(number_samples):\n",
    "#     filtered_lfp[i] /= np.max(np.abs(filtered_lfp[i]))\n",
    "    fst_idx = first_pk_tr(filtered_lfp[i])\n",
    "    start, end = get_spike_window(filtered_lfp[i], win_size=params.WINDOW_SIZE, align_at=pk_tr_idx_in_window)\n",
    "    lfp_list.append(filtered_lfp[i,start:end,:])\n",
    "    window_range.append((start, end))\n",
    "\n",
    "windowed_lfp = np.stack(lfp_list, axis=0)  # (samples x time window x channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-vocabulary",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import Output, HBox, VBox, Label, Layout\n",
    "\n",
    "ix = 1  # ix-th column of electrode along x-axis\n",
    "# vlim = 'auto'  # heatmap color limit\n",
    "vlim = 0.002*np.array([-1.,1.])\n",
    "\n",
    "fig_properties = {'fontsize': 15, 'labelpad': 0, 'ticksize': 10, 'nbins': 5}\n",
    "\n",
    "t = sim.t()\n",
    "x_dist = np.unique(params.ELECTRODE_POSITION[:,0])\n",
    "elec_pos = params.ELECTRODE_POSITION\n",
    "e_idx = elec_pos[:,0]==x_dist[ix]\n",
    "\n",
    "print('Using column at x = %g um' % (x_dist[ix]))\n",
    "\n",
    "outputs = []\n",
    "for i in range(number_samples):\n",
    "    out = [Output() for i in range(3)]\n",
    "    with out[0]:\n",
    "        _ = plot_lfp_heatmap(t, elec_pos[e_idx, 1], lfp[i][:,e_idx], \n",
    "                             vlim=vlim, **fig_properties)\n",
    "    with out[1]:\n",
    "            _ = plot_lfp_heatmap(t, elec_pos[e_idx, 1], filtered_lfp[i][:,e_idx],\n",
    "                             vlim=vlim, **fig_properties)\n",
    "    with out[2]:\n",
    "        tidx = slice(window_range[i][0],window_range[i][1])\n",
    "        _ = plot_lfp_heatmap(t[tidx], elec_pos[e_idx, 1], windowed_lfp[i][:,e_idx],\n",
    "                             vlim=vlim, **fig_properties)\n",
    "    outputs.append(out)\n",
    "\n",
    "around = Layout(justify_content='space-around')\n",
    "out = VBox( [HBox( [Label('Raw LFP'), Label('Filtered LFP'), Label('Windowed LFP')], layout=around )]\n",
    "          + [HBox(out, layout=around) for i,out in enumerate(outputs)] )\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f3b995",
   "metadata": {},
   "source": [
    "### Get summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_stats = []\n",
    "for i in range(number_samples):\n",
    "    g_lfp, _ = build_lfp_grid(windowed_lfp[i], params.ELECTRODE_POSITION)\n",
    "    summ_stats.append(calculate_stats(g_lfp))\n",
    "summ_stats = np.array(summ_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_along_time = ['avg', 'rel_t', 'std', 'trough', 'peak']\n",
    "stats_along_xy = ['mean', 'std', 'max_idx_x', 'max_idx_y', 'max_val', 'min_idx_x', 'min_idx_y', 'min_val']\n",
    "i_min = 2  # include minimum statistics for the first i_min in stats_list\n",
    "extra_stats = ['t0', 't1', 't2',\n",
    "               't0_idx_y_left', 't0_idx_y_right',\n",
    "               't2_idx_y_left', 't2_idx_y_right',\n",
    "               't1_max_idx_y', 't1_min_idx_y']\n",
    "\n",
    "summ_stats_names = []\n",
    "for i, st in enumerate(stats_along_time):\n",
    "    stats_xy = stats_along_xy if i<i_min else stats_along_xy[:-3]\n",
    "    for sxy in stats_xy:\n",
    "        summ_stats_names.append('__'.join([st,sxy]))\n",
    "summ_stats_names += extra_stats\n",
    "print(\"%d summary stats\" % (len(summ_stats_names)))\n",
    "print(json.dumps(summ_stats_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-arbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_stats_df = pd.DataFrame(summ_stats, columns=summ_stats_names)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "display(summ_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-turkish",
   "metadata": {},
   "outputs": [],
   "source": [
    "varied_parameter_column = all_param['x']\n",
    "correlations1 = {column: [varied_parameter_column.corr(summ_stats_df[column])] for column in summ_stats_df}\n",
    "# correlations1 = dict(sorted(correlations1.items(), key=lambda item: item[1]))\n",
    "# print(json.dumps(correlations1, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "varied_parameter_column = all_param['y']\n",
    "correlations2 = {column: [varied_parameter_column.corr(summ_stats_df[column])] for column in summ_stats_df}\n",
    "# correlations2 = dict(sorted(correlations2.items(), key=lambda item: item[1]))\n",
    "# print(json.dumps(correlations2, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "varied_parameter_column = all_param['z']\n",
    "correlations3 = {column: [varied_parameter_column.corr(summ_stats_df[column])] for column in summ_stats_df}\n",
    "# correlations3 = dict(sorted(correlations3.items(), key=lambda item: item[1]))\n",
    "# print(json.dumps(correlations3, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "varied_parameter_column = all_param['h']\n",
    "correlations4 = {column: [varied_parameter_column.corr(summ_stats_df[column])] for column in summ_stats_df}\n",
    "# correlations3 = dict(sorted(correlations3.items(), key=lambda item: item[1]))\n",
    "# print(json.dumps(correlations3, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "varied_parameter_column = all_param['phi']\n",
    "correlations5 = {column: [varied_parameter_column.corr(summ_stats_df[column])] for column in summ_stats_df}\n",
    "# correlations3 = dict(sorted(correlations3.items(), key=lambda item: item[1]))\n",
    "# print(json.dumps(correlations3, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "varied_parameter_column = all_param['l_t']\n",
    "correlations6 = {column: [varied_parameter_column.corr(summ_stats_df[column])] for column in summ_stats_df}\n",
    "# correlations3 = dict(sorted(correlations3.items(), key=lambda item: item[1]))\n",
    "# print(json.dumps(correlations3, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = {}\n",
    "\n",
    "for k, v in correlations1.items():\n",
    "    combined[k] = [v[0], correlations2[k][0], correlations3[k][0], correlations4[k][0], correlations5[k][0], correlations6[k][0]]\n",
    "    \n",
    "combined_stats = pd.DataFrame.from_dict(combined)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "display(combined_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-retail",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
